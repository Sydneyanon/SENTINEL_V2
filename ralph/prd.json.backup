{
  "project": "Prometheus Signals Bot",
  "branchName": "ralph/optimize-v1",
  "description": "Autonomous optimization of Prometheus memecoin signals bot - AGGRESSIVE MODE: Target 75% win rate",
  "goal": "75% win rate using any optimization necessary",
  "userStories": [
    {
      "id": "OPT-000",
      "title": "EMERGENCY: Kill all losing signals immediately",
      "description": "As a trader, I want to stop posting signals that consistently lose so we only post winners",
      "acceptanceCriteria": [
        "Query database for all signals from last 7 days with outcomes",
        "Calculate win rate per combination: (KOL tier + narrative + holder pattern)",
        "Blacklist any pattern with <40% win rate",
        "Add signal_pattern_filter.py that blocks low-performing patterns",
        "Test for 2 hours: measure if blocked signals would have lost",
        "Keep if prevents >3 losing signals OR improves win rate >5%"
      ],
      "priority": 0,
      "passes": false,
      "notes": "HIGHEST PRIORITY - Cut losses first before optimizing winners",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-001",
      "title": "Optimize conviction score threshold",
      "description": "As a trader, I want higher quality signals by tuning the minimum conviction threshold",
      "acceptanceCriteria": [
        "Test conviction thresholds: 65, 70, 75, 80",
        "Monitor for 2 hours after deploy to Railway",
        "Measure: signal count, avg ROI, false positive rate",
        "Keep threshold that maximizes (ROI * signal_count) while keeping false positives < 30%",
        "Update config.py with optimal value",
        "Commit if metrics improve by >10%"
      ],
      "priority": 1,
      "passes": false,
      "notes": "",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-002",
      "title": "Reduce Helius API credit waste",
      "description": "As an operator, I want to minimize Helius credit usage without sacrificing signal quality",
      "acceptanceCriteria": [
        "Identify top 3 credit-consuming operations from logs",
        "Increase caching TTL for holder checks (60min â†’ 120min)",
        "Monitor credit usage for 2 hours",
        "Ensure signal quality doesn't drop >5%",
        "Commit if credits reduced by >20%"
      ],
      "priority": 2,
      "passes": false,
      "notes": "",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-003",
      "title": "Auto-discover high-performing wallets",
      "description": "As a system, I want to automatically add wallets that consistently call winners",
      "acceptanceCriteria": [
        "Query database for wallets that appeared in 3+ successful signals (>2x ROI)",
        "Filter for win rate >70%",
        "Add top 5 to curated_wallets.py as 'discovered' tier",
        "Monitor for 4 hours",
        "Keep if new wallets generate >1 successful signal",
        "Commit wallet additions with performance data"
      ],
      "priority": 3,
      "passes": false,
      "notes": "",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-004",
      "title": "Tune bundle detection penalties",
      "description": "As a filter, I want optimal bundle penalties that catch rugs without false positives",
      "acceptanceCriteria": [
        "Test bundle penalty values: -10, -15, -20 for minor bundles",
        "Monitor rug detection accuracy for 2 hours",
        "Measure: rugs caught, false positives (organic pumps blocked)",
        "Optimize for max rugs caught with <10% false positive rate",
        "Update RUG_DETECTION config",
        "Commit if rug catch rate improves >15%"
      ],
      "priority": 4,
      "passes": false,
      "notes": "",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-005",
      "title": "Build ML prediction layer",
      "description": "As a system, I want to learn from historical signals to predict success",
      "acceptanceCriteria": [
        "Extract features from database: smart_wallet_count, narrative_match, holder_concentration, volume_ratio",
        "Train simple logistic regression on historical signals (target: 2x+ ROI)",
        "Add ML score as bonus points (0-10) in conviction_engine.py",
        "Monitor for 4 hours",
        "Keep if signal success rate improves >10%",
        "Commit model weights and integration code"
      ],
      "priority": 5,
      "passes": false,
      "notes": "",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-006",
      "title": "Build on-chain data pipeline for proprietary signals",
      "description": "As a system, I want real-time Solana on-chain data to detect smart money moves before public APIs",
      "acceptanceCriteria": [
        "Implement Helius Geyser websocket streamer for new token launches (Pump.fun)",
        "Store 10k+ transactions to database (wallet, token, amount, timestamp)",
        "Add data processing: cluster wallets by behavior (early buyers, quick flippers)",
        "Create new signal source: 'on_chain_clusters' in conviction engine",
        "Monitor for 4 hours",
        "Keep if discovers 1+ high-performing wallet not in curated list",
        "Commit streaming code + database schema"
      ],
      "priority": 6,
      "passes": false,
      "notes": "Grok idea: Build data moat with on-chain streaming",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-007",
      "title": "Add RSS narrative detection with BERTopic ML",
      "description": "As a narrative detector, I want to discover emerging narratives from crypto news without X API costs",
      "acceptanceCriteria": [
        "Implement RSS feed ingestion: CoinDesk, CoinTelegraph, Decrypt (free)",
        "Add BERTopic clustering: hourly analysis for topic trends",
        "Detect 'emerging' narratives: new clusters with >5 mentions in 24h",
        "Add Reddit PRAW scraper for r/Solana hot posts (optional)",
        "Auto-update HOT_NARRATIVES in config.py with discovered topics",
        "Monitor for 24 hours",
        "Keep if discovers 1+ narrative that generates successful signal",
        "Commit RSS pipeline + BERTopic model"
      ],
      "priority": 7,
      "passes": false,
      "notes": "Grok idea: Free narrative data via RSS + ML clustering",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-008",
      "title": "ML-powered smart wallet discovery from on-chain data",
      "description": "As a wallet curator, I want ML to automatically find high-alpha wallets from transaction patterns",
      "acceptanceCriteria": [
        "Query Dune/Helius for Solana traders: 100+ trades, meme focus",
        "Extract features: win_rate, avg_roi, hold_time, entry_timing, trade_frequency",
        "Train scikit-learn classifier: 'smart' = win_rate>70% AND avg_roi>50%",
        "Apply to unlabeled wallets: predict top 20 candidates",
        "Add top 5 to curated_wallets.py as 'ml_discovered' tier",
        "Monitor for 7 days",
        "Keep if ML wallets generate 2+ successful signals",
        "Commit ML model + discovered wallets"
      ],
      "priority": 8,
      "passes": false,
      "notes": "Grok idea: Proprietary smart wallet list via ML clustering",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-009",
      "title": "Build backtesting framework for strategy validation",
      "description": "As a system, I want to validate optimizations on historical data before deploying",
      "acceptanceCriteria": [
        "Create backtesting engine: replay historical signals from database",
        "Simulate conviction scoring with different thresholds/weights",
        "Measure historical ROI, win_rate, drawdowns for each config",
        "Add test suite: backtest OPT-001 through OPT-005 on last 30 days",
        "Generate report: best config = highest Sharpe ratio",
        "Integrate into ralph/collect_metrics.py for future optimizations",
        "Commit backtesting framework"
      ],
      "priority": 9,
      "passes": false,
      "notes": "Grok idea: Backtest optimizations before live deploy",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-010",
      "title": "Add dynamic risk management with ML exit strategies",
      "description": "As a trader, I want automated TP/SL that adapts to volatility and market conditions",
      "acceptanceCriteria": [
        "Implement volatility calculation: rolling std dev from price data",
        "Train ML model for dynamic exits: features (volatility, volume_spike, holder_growth)",
        "Add adaptive trailing stop: tighter in high volatility, wider in calm markets",
        "Create exit signal generator in conviction_engine.py",
        "Backtest on 100 historical signals: measure improvement in avg_roi",
        "Monitor live for 7 days",
        "Keep if avg_roi improves >15% or max drawdown reduces >20%",
        "Commit ML exit model + integration"
      ],
      "priority": 10,
      "passes": false,
      "notes": "Grok idea: ML-powered dynamic TP/SL for better exits",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-011",
      "title": "Track Telegram call group accuracy (like outlight.fun)",
      "description": "As a signal source, I want to monitor Telegram groups and score them by accuracy",
      "acceptanceCriteria": [
        "Use Telethon/Pyrogram to monitor 10-20 popular Solana call groups",
        "Parse call messages: extract token CA, timestamp, entry price",
        "Track outcomes: did it pump? max ROI reached?",
        "Store to database: group_id, token, call_time, outcome, ROI",
        "Calculate caller accuracy: % of calls that 2x'd, avg ROI",
        "Add top 3 accurate groups as signal source in conviction engine (+5-15 pts)",
        "Monitor for 7 days",
        "Keep if group calls generate 2+ successful signals",
        "Commit Telegram monitoring code + caller scoring"
      ],
      "priority": 11,
      "passes": false,
      "notes": "Replicate outlight.fun caller tracking - monitor TG groups, score by accuracy",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-012",
      "title": "Build ML learning engine to identify high market cap conditions",
      "description": "As a learning system, I want to understand what conditions lead to tokens hitting high market caps (100k+, 1M+, 10M+)",
      "acceptanceCriteria": [
        "Extract features from all tracked tokens: holder_concentration_top10, holder_concentration_top3, volume_to_mcap_ratio, kol_count, kol_tier_distribution, unique_buyers_first_hour, price_momentum_5m, price_momentum_1h, liquidity_depth, narrative_match_count, bundle_detected, early_buyer_count, avg_entry_time, holder_growth_rate",
        "Label outcomes: 0=rug, 1=2x, 2=10x, 3=50x, 4=100x+",
        "Train gradient boosting model (XGBoost) to predict outcome class",
        "Feature importance analysis: which signals matter most for 100x?",
        "Generate insights: 'Tokens with 3+ elite KOLs + <30% top-10 holders + AI narrative = 65% chance of 10x+'",
        "Add ML predictions to conviction engine: +0-20 points based on predicted outcome",
        "Retrain weekly on new data (continuous learning)",
        "Monitor for 14 days",
        "Keep if prediction accuracy >70% and signals improve >15%",
        "Commit ML model, feature extractor, training pipeline, insights dashboard"
      ],
      "priority": 12,
      "passes": false,
      "notes": "Meta-learning: Learn what conditions = high mcap. Features: holders%, volume, KOLs, narratives, momentum, etc.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-013",
      "title": "Auto-detect and fix runtime errors from Railway logs",
      "description": "As a self-healing system, I want to detect errors in Railway logs and automatically fix them",
      "acceptanceCriteria": [
        "Implement Railway API log fetcher: pull last 1000 lines of logs",
        "Parse for error patterns: 'ERROR', 'Exception', 'Failed', 'Traceback'",
        "Classify errors: API failures, data parsing, missing data, timeouts",
        "For each error type, implement fix: add try/catch, add validation, add fallbacks",
        "Test fixes locally, then deploy to Railway",
        "Monitor for 2 hours to ensure error rate drops",
        "Keep if error count decreases >50%",
        "Commit fixes with error analysis in commit message"
      ],
      "priority": 13,
      "passes": false,
      "notes": "User request: Ralph should fix errors automatically. High priority!",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-014",
      "title": "Optimize metadata collection (name, symbol, description)",
      "description": "As a data pipeline, I want faster and more reliable token metadata without wasting credits",
      "acceptanceCriteria": [
        "Audit current metadata sources: Helius DAS API, bonding curve, DexScreener",
        "Measure success rate and latency for each source",
        "Add fallback chain: Helius â†’ DexScreener â†’ Jupiter API â†’ Solscan",
        "Implement 24h cache for token metadata (Redis or in-memory)",
        "Add parallel fetching: fetch metadata + price + holders simultaneously",
        "Monitor for 2 hours",
        "Keep if metadata fetch success rate >95% and latency <500ms",
        "Commit optimized metadata pipeline"
      ],
      "priority": 14,
      "passes": false,
      "notes": "User request: Optimize metadata collection. Name/symbol often missing.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-015",
      "title": "Optimize price data fetching (reduce 'No price data' errors)",
      "description": "As a pricing system, I want accurate prices from multiple sources with smart fallbacks",
      "acceptanceCriteria": [
        "Audit price sources: bonding curve decoder, DexScreener, Jupiter, Raydium",
        "Measure success rate per source and avg price accuracy",
        "Implement weighted price aggregation: average prices from multiple sources",
        "Add staleness detection: reject prices >5 minutes old",
        "Implement price cache: 30s TTL to reduce API calls",
        "Add sanity checks: reject outliers (price changes >500% in 1 min)",
        "Monitor for 2 hours",
        "Keep if 'No price data' errors drop >80%",
        "Commit improved price pipeline"
      ],
      "priority": 15,
      "passes": false,
      "notes": "User request: Fix price fetching. Often shows 'No price data available'.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-016",
      "title": "Track and optimize KOL performance (win rate, ROI, accuracy)",
      "description": "As a wallet curator, I want to track which KOLs are actually making money and adjust scoring",
      "acceptanceCriteria": [
        "For each KOL wallet, track: tokens bought, outcomes (rug/2x/10x/50x+), win_rate, avg_roi",
        "Store to database: kol_performance table with weekly aggregates",
        "Auto-demote KOLs with <50% win rate after 20+ trades",
        "Auto-promote wallets with >75% win rate + >3x avg ROI",
        "Adjust scoring weights: high-performing KOLs get +15 pts, low-performers get +5 pts",
        "Create /kol-leaderboard command: show top 10 KOLs by performance",
        "Monitor for 7 days",
        "Keep if signal quality improves >10%",
        "Commit KOL performance tracking system"
      ],
      "priority": 16,
      "passes": false,
      "notes": "User request: Track KOL win rates and optimize scoring based on actual performance.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-017",
      "title": "Auto-tune scoring weights with ML gradient descent",
      "description": "As a self-optimizing system, I want to learn optimal scoring weights from historical data",
      "acceptanceCriteria": [
        "Extract last 100 signals from database with outcomes (rug/2x/10x/etc)",
        "Current weights: smart_wallet=40, narrative=25, holders=15, volume=10, momentum=10",
        "Train ML model (gradient descent) to predict outcome from feature scores",
        "Find optimal weights that maximize 10x+ predictions while minimizing rugs",
        "Test on validation set: compare old weights vs new weights",
        "Apply new weights to config.py",
        "Monitor for 3 days",
        "Keep if 10x+ rate improves >20% OR rug rate drops >30%",
        "Commit optimized weights with ML analysis report"
      ],
      "priority": 17,
      "passes": false,
      "notes": "User request: Optimize scoring algorithm. Let ML find best weights.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-018",
      "title": "SPEED: Rapid A/B test top 5 conviction thresholds",
      "description": "As an optimizer, I want to test multiple thresholds in parallel to find the winner fast",
      "acceptanceCriteria": [
        "Test conviction thresholds: 60, 65, 70, 75, 80 simultaneously",
        "Deploy 5 branches, collect metrics for 1 hour each in parallel",
        "Compare: signal_count, win_rate, avg_ROI, false_positives",
        "Pick winner = highest (win_rate * avg_ROI) with signal_count >= 5",
        "Deploy winner to main immediately",
        "Document in progress.txt with data table"
      ],
      "priority": 2,
      "passes": false,
      "notes": "AGGRESSIVE: Parallel testing to find optimal threshold in 1 hour instead of 10 hours",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-019",
      "title": "KILL: Auto-blacklist consistently wrong KOLs",
      "description": "As a curator, I want to stop following KOLs who keep buying rugs",
      "acceptanceCriteria": [
        "Query last 30 days: KOL wallet performance (win_rate, avg_ROI, rug_rate)",
        "Auto-demote KOLs with: win_rate < 35% OR rug_rate > 60%",
        "Move them from elite/god tier to 'watchlist' (0 points)",
        "Monitor for 24 hours",
        "Keep if win_rate improves >8% OR rug_rate drops >15%",
        "Log demoted wallets to progress.txt"
      ],
      "priority": 1,
      "passes": false,
      "notes": "AGGRESSIVE: Cut dead weight. Bad KOLs hurt win rate more than good KOLs help.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-020",
      "title": "DOUBLE DOWN: 2x points for proven winners",
      "description": "As a scoring system, I want to amplify signals from wallets with >70% win rate",
      "acceptanceCriteria": [
        "Calculate 30-day win rate for each KOL wallet",
        "Wallets with win_rate >= 70%: DOUBLE their conviction points",
        "Wallets with win_rate >= 80%: TRIPLE their points",
        "Wallets with win_rate < 40%: HALVE their points",
        "Monitor for 12 hours",
        "Keep if overall win_rate improves >10%"
      ],
      "priority": 3,
      "passes": false,
      "notes": "AGGRESSIVE: Amplify winners, suppress losers. Data-driven weighting.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-021",
      "title": "NARRATIVE BOOST: 3x multiplier for hot narratives",
      "description": "As a narrative detector, I want massive conviction boost for trending narratives",
      "acceptanceCriteria": [
        "Check which narratives generated 5+ winning signals in last 48h",
        "Apply 3x conviction multiplier to those narratives",
        "Narratives with 0 wins in 7 days: 0.5x penalty",
        "Monitor for 6 hours",
        "Keep if win_rate on narrative signals improves >15%"
      ],
      "priority": 4,
      "passes": false,
      "notes": "AGGRESSIVE: Ride winning narratives hard, avoid dead narratives",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-022",
      "title": "TIMING: Only post signals in high-activity hours",
      "description": "As a strategist, I want to post signals when degen traders are most active",
      "acceptanceCriteria": [
        "Analyze historical signal performance by hour of day (UTC)",
        "Identify top 3 time windows with highest win_rate",
        "Block signals outside these windows (queue for next window)",
        "Monitor for 24 hours",
        "Keep if win_rate improves >12%"
      ],
      "priority": 5,
      "passes": false,
      "notes": "AGGRESSIVE: Timing matters. Post when market is hot.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-023",
      "title": "EMERGENCY STOP: Kill signals with red flags",
      "description": "As a risk manager, I want to block signals with obvious rug indicators",
      "acceptanceCriteria": [
        "Add instant blocklist: top 3 holders >80%, liquidity <$5k, no socials",
        "Block if bonding curve shows suspicious pattern",
        "Block if token age <2 minutes (too fresh)",
        "Add early exit for KOL wallets that bought then sold in <5min",
        "Monitor for 4 hours",
        "Keep if rug_rate drops >25%"
      ],
      "priority": 1,
      "passes": false,
      "notes": "IMPLEMENTED - Waiting for deployment. Commit: ade591b. Added emergency stop checks: top holders >80%, liquidity <$5k, token age <2min, zero liquidity pre-grad. Paranoid filtering to prevent obvious rugs.",
      "baseline_metrics": {},
      "implementation_status": "deployed_pending_monitoring",
      "deployment_commit": "ade591b",
      "monitoring_start": null,
      "monitoring_duration_hours": 4
    },
    {
      "id": "OPT-024",
      "title": "CONVICTION FLOOR: Raise minimum to 75",
      "description": "As a quality filter, I want only the highest conviction signals",
      "acceptanceCriteria": [
        "Change MIN_CONVICTION_SCORE from current to 75",
        "Monitor for 3 hours",
        "Measure: signal_count (expect ~50% drop), win_rate (expect >60%), avg_ROI",
        "Keep if: (win_rate > 60% AND avg_ROI > 3x) OR (win_rate > 70%)",
        "Revert if signal_count < 2 per hour (too restrictive)"
      ],
      "priority": 2,
      "passes": false,
      "notes": "AGGRESSIVE: Quality over quantity. Aim for 75%+ win rate with high conviction only.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-025",
      "title": "ML PREDICTIONS: Add XGBoost win probability",
      "description": "As an AI system, I want ML predictions to boost high-probability winners",
      "acceptanceCriteria": [
        "Train XGBoost on last 100 signals with features: kol_count, holder_pattern, narrative_strength, volume, liquidity",
        "Predict win probability (0-1) for each signal",
        "Add conviction bonus: +20 pts if P(win) > 0.75, +0 if P(win) < 0.5",
        "Only post signals with P(win) > 0.60",
        "Monitor for 8 hours",
        "Keep if win_rate improves >15% AND model accuracy >65%"
      ],
      "priority": 6,
      "passes": false,
      "notes": "AGGRESSIVE: Use ML to filter out likely losers before posting",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-026",
      "title": "COPY TOP PERFORMERS: Mirror best KOL completely",
      "description": "As a follower, I want to copy the single best KOL's every move",
      "acceptanceCriteria": [
        "Identify KOL with highest 30-day win_rate (min 10 trades)",
        "Auto-post every token they buy within 60 seconds",
        "Bypass conviction scoring for this wallet (instant signals)",
        "Monitor for 24 hours",
        "Keep if win_rate on these signals > 65%"
      ],
      "priority": 7,
      "passes": false,
      "notes": "AGGRESSIVE: If one KOL is crushing it, copy them exactly",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-027",
      "title": "UX: Display KOL names in Telegram posts",
      "description": "As a trader, I want to see which KOLs bought so I know who's behind the signal",
      "acceptanceCriteria": [
        "Add KOL name/label to Telegram embed (e.g., 'Ansem', 'Machi Big Brother')",
        "Show tier badge (ðŸ”¥ Elite, ðŸ‘‘ God, etc.)",
        "If multiple KOLs: show top 3 by tier",
        "Pull names from curated_wallets.py metadata",
        "Test Telegram formatting looks clean",
        "Deploy and monitor for formatting issues"
      ],
      "priority": 4,
      "passes": false,
      "notes": "USER REQUEST: Make signals more transparent and trustworthy",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-028",
      "title": "FIX: Telegram channel scraper calling issues",
      "description": "As a monitor, I want reliable Telegram channel tracking without missing calls",
      "acceptanceCriteria": [
        "Debug telegram_monitor.py - check for connection drops, auth issues",
        "Add reconnection logic with exponential backoff",
        "Add health check: alert if no messages received in 10min",
        "Log all calls to database with timestamp and channel",
        "Monitor for 6 hours - ensure no missed calls",
        "Keep if: no connection drops AND all channels tracked reliably"
      ],
      "priority": 3,
      "passes": false,
      "notes": "USER REQUEST: Fix reliability issues with Telegram scraper",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-029",
      "title": "GROWTH: Auto-discover high win-rate Telegram channels",
      "description": "As a researcher, I want to find new alpha channels with proven track records",
      "acceptanceCriteria": [
        "Scrape popular Solana Telegram directories for memecoin channels",
        "Monitor each channel for 7 days, track all token calls",
        "Calculate win rate per channel (track token price 6h/24h after call)",
        "Auto-add channels with: win_rate >55% AND >20 calls tracked",
        "Weight Telegram signals from high-performing channels higher",
        "Keep if: new channels add >3 winning signals per week"
      ],
      "priority": 5,
      "passes": false,
      "notes": "USER REQUEST: Find new high-quality alpha sources automatically",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-030",
      "title": "GROWTH: Auto-discover and add new KOL wallets",
      "description": "As a wallet tracker, I want to find emerging KOLs before they're widely known",
      "acceptanceCriteria": [
        "Track wallets mentioned in high-performing Telegram channels",
        "Monitor whale wallets from DexScreener top traders",
        "Calculate 30-day performance: win_rate, avg_ROI, trade_frequency",
        "Auto-promote wallets with: win_rate >65% AND avg_ROI >4x AND >15 trades",
        "Start them at 'promising' tier (low points), upgrade if they maintain performance",
        "Monitor for 14 days",
        "Keep if: new KOLs contribute >2 winning signals per week"
      ],
      "priority": 6,
      "passes": false,
      "notes": "USER REQUEST: Expand KOL list automatically with data-driven discovery",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-031",
      "title": "REAL-TIME: Automated narrative detection and updates",
      "description": "As a narrative tracker, I want narratives to update automatically based on trending tokens",
      "acceptanceCriteria": [
        "Scan token names/descriptions for keywords every 15 minutes",
        "Track keyword frequency: 'AI', 'cat', 'dog', 'pepe', 'degen', etc.",
        "Auto-create new narrative if keyword appears in >5 tokens in 1 hour",
        "Calculate narrative win_rate dynamically (last 24h performance)",
        "Boost trending narratives (+15 conviction pts), suppress dead ones (-10 pts)",
        "Monitor for 24 hours",
        "Keep if: catches trending narratives >6 hours faster than manual"
      ],
      "priority": 4,
      "passes": false,
      "notes": "USER REQUEST: Real-time narrative detection instead of static list",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-032",
      "title": "EXIT STRATEGY: Smart profit-taking system",
      "description": "As a trader, I want the bot to tell me when to take profits based on data",
      "acceptanceCriteria": [
        "Track all posted signals: price, volume, holder count, liquidity",
        "Define exit signals: volume drops >70%, holders drop >30%, or price 5x+",
        "Calculate exit timing: if hit 5x within 2h, suggest 50% exit at 3x (take profits)",
        "Build exit score: 0-100 based on sustainability indicators",
        "Test on historical signals: would exit strategy improve realized gains?",
        "Keep if: backtested exit strategy improves avg realized ROI >20%"
      ],
      "priority": 5,
      "passes": false,
      "notes": "USER REQUEST: Help traders know when to exit, not just when to enter",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-033",
      "title": "EXIT ALERTS: Telegram posts for when to sell",
      "description": "As a trader, I want Telegram alerts telling me when to take profits on active positions",
      "acceptanceCriteria": [
        "Monitor all posted signals continuously (price, volume, holders)",
        "Post exit alert when: hit 3x ('Take 50% profits'), 5x ('Take 70%'), or red flags ('EXIT NOW')",
        "Red flags: volume crashed >80%, holders down >40%, liquidity pulled >50%",
        "Format: 'ðŸš¨ EXIT SIGNAL: [TOKEN] hit 5x - Suggest taking 70% profits. Volume: -15%, Holders: +5%'",
        "Track if users follow exit signals (measure via on-chain)",
        "Monitor for 48 hours",
        "Keep if: exit alerts would have improved avg ROI >15% (vs holding)"
      ],
      "priority": 3,
      "passes": false,
      "notes": "USER REQUEST: Active management - tell traders when to exit positions",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-034",
      "title": "TIMING: Analyze optimal hours/days for signals",
      "description": "As a strategist, I want to know WHEN tokens are most likely to run so we push hard at the right times",
      "acceptanceCriteria": [
        "Query database: all signals with outcomes, group by hour (0-23 UTC) and day (Mon-Sun)",
        "Calculate win_rate, avg_ROI, rug_rate per time slot",
        "Identify HOT ZONES: times with >65% WR (push hard - post everything)",
        "Identify COLD ZONES: times with <45% WR (ease off - only post 80+ conviction)",
        "Create time_optimizer.py that adjusts MIN_CONVICTION dynamically",
        "Monitor for 48 hours",
        "Keep if: win_rate improves >8% by being selective in bad times"
      ],
      "priority": 1,
      "passes": false,
      "notes": "USER REQUEST: Push hard when optimal, ease off otherwise. Time of day/week matters for degen activity.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-035",
      "title": "SPEED: Catch tokens within 60 seconds of KOL buy",
      "description": "As a sniper, I want to post signals FAST before price pumps",
      "acceptanceCriteria": [
        "Measure current latency: KOL buy â†’ signal posted (avg time)",
        "Optimize websocket processing: parallel instead of sequential",
        "Cache metadata aggressively (avoid redundant Helius calls)",
        "Target: <60 seconds from KOL buy to Telegram post",
        "Monitor for 6 hours",
        "Keep if: avg_ROI improves >15% (earlier entry = better prices)"
      ],
      "priority": 2,
      "passes": false,
      "notes": "CRITICAL: Speed matters. Faster signals = better entry prices = higher ROI.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-036",
      "title": "DATA QUALITY: Never post signals with missing critical data",
      "description": "As a quality filter, I want to block signals when we don't have reliable data",
      "acceptanceCriteria": [
        "Add data quality checks: price, liquidity, holder_count must exist",
        "Block signal if: price = 0 or None, liquidity < $1k, holder_count = 0 (pre-grad exempt)",
        "Block if API errors on critical data (Helius timeout, DexScreener 404)",
        "Log blocked signals with reason",
        "Monitor for 4 hours",
        "Keep if: rug_rate drops >10% (bad data = rugs)"
      ],
      "priority": 1,
      "passes": false,
      "notes": "IMPLEMENTED - Waiting for deployment and monitoring. Commit: 136013d. Added strict data quality checks: price > 0, liquidity >= $1k, mcap > 0, holder_count > 0 (post-grad). Blocks bad signals before posting.",
      "baseline_metrics": {},
      "implementation_status": "deployed_pending_monitoring",
      "deployment_commit": "136013d",
      "monitoring_start": null,
      "monitoring_duration_hours": 4
    },
    {
      "id": "OPT-037",
      "title": "LEARNING: Build rug pattern database from past failures",
      "description": "As a learner, I want to remember every rug so we never fall for the same pattern twice",
      "acceptanceCriteria": [
        "Query all signals with outcome='rug' from last 30 days",
        "Extract features: holder pattern, liquidity curve, volume pattern, wallet behavior",
        "Build rug fingerprint database (JSON file)",
        "Add rug_pattern_matcher.py that checks new signals against known rug patterns",
        "Score similarity 0-100, block if >80% match to known rug",
        "Monitor for 24 hours",
        "Keep if: prevents >5 rugs OR rug_rate drops >20%"
      ],
      "priority": 2,
      "passes": false,
      "notes": "CRITICAL: Learn from mistakes. Don't post similar rugs twice. Build institutional memory.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-038",
      "title": "MARKET CONDITIONS: Detect pump.fun meta shifts",
      "description": "As a market analyst, I want to know when overall market is hot/cold and adjust strategy",
      "acceptanceCriteria": [
        "Track pump.fun metrics every 15min: new token count, volume, graduation rate",
        "Define HOT MARKET: >200 new tokens/hour, >$5M volume/hour (post aggressively)",
        "Define COLD MARKET: <50 new tokens/hour, <$1M volume/hour (only elite signals)",
        "Adjust conviction threshold dynamically: -10 pts in hot markets, +15 pts in cold",
        "Monitor for 24 hours",
        "Keep if: win_rate improves >10% by being selective when market is cold"
      ],
      "priority": 3,
      "passes": false,
      "notes": "ADVANCED: Ride the wave when market is hot, hide when it's cold. Meta awareness.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-039",
      "title": "HOLDER QUALITY: Analyze wallet quality not just count",
      "description": "As a holder analyzer, I want to know if holders are real traders or bots/bundlers",
      "acceptanceCriteria": [
        "For each token, check top 20 holders: are they real wallets or suspicious?",
        "Suspicious indicators: new wallet (<7 days), only holds this token, part of bundle",
        "Calculate quality_score: % of holders that are legitimate traders",
        "Penalty: -20 conviction pts if <50% quality holders",
        "Bonus: +15 pts if >80% quality holders (real accumulation)",
        "Monitor for 8 hours",
        "Keep if: rug_rate drops >15% (bot holders = red flag)"
      ],
      "priority": 4,
      "passes": false,
      "notes": "ADVANCED: 1000 bot holders != 100 real trader holders. Quality over quantity.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-040",
      "title": "CONFIRMATION: Require 2+ KOLs for risky tokens",
      "description": "As a risk manager, I want multiple KOL confirmation on sketchy tokens",
      "acceptanceCriteria": [
        "Define RISKY: new token (<5min old), low liquidity (<$10k), or suspicious holders",
        "For risky tokens: require 2+ elite KOLs OR 1 god-tier KOL before posting",
        "For safe tokens: 1 KOL is enough",
        "Add 'waiting for confirmation' state (max 3min wait)",
        "Monitor for 6 hours",
        "Keep if: false positive rate drops >25% (stop posting solo-KOL rugs)"
      ],
      "priority": 3,
      "passes": false,
      "notes": "CRITICAL: Single KOL buying sus token = risky. Wait for confirmation from others.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-041",
      "title": "COST: Eliminate redundant Helius API calls",
      "description": "As an operator, I want to stop wasting Helius credits on duplicate/unnecessary calls",
      "acceptanceCriteria": [
        "Audit code: find all Helius API calls (holder checks, metadata, transactions)",
        "Identify redundant calls: same token checked multiple times in 5min window",
        "Implement aggressive caching: 2h TTL for holder data, 1h for metadata",
        "Add request deduplication: batch multiple checks into single call",
        "Measure credits used per signal (before vs after)",
        "Monitor for 6 hours",
        "Keep if: credits per signal drops >40% with no quality loss"
      ],
      "priority": 4,
      "passes": false,
      "notes": "INFRASTRUCTURE: We've wasted tons of credits. Every call costs money. Optimize ruthlessly.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-042",
      "title": "INFRASTRUCTURE: Auto-fix Railway crashes and errors",
      "description": "As a self-healing system, I want to detect and fix runtime errors automatically",
      "acceptanceCriteria": [
        "Monitor Railway logs every 5min for ERROR/Exception/Fatal",
        "Classify errors: logging spam (add rate limit), API timeout (add retry), memory leak (restart service)",
        "Auto-apply fixes: reduce log level, increase timeout, add circuit breaker",
        "Track error frequency before/after fixes",
        "Monitor for 12 hours",
        "Keep if: error rate drops >60% OR no container restarts in 24h"
      ],
      "priority": 2,
      "passes": false,
      "notes": "CRITICAL: We've fought logging spam and crashes. Ralph should fix infrastructure issues proactively.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-043",
      "title": "DYNAMIC CONFIDENCE: Adjust strategy based on recent performance",
      "description": "As an adaptive system, I want to be conservative after losses and aggressive after wins",
      "acceptanceCriteria": [
        "Track last 10 signals: calculate rolling win_rate",
        "If last 10 signals: >70% WR â†’ AGGRESSIVE MODE (lower threshold by 10 pts)",
        "If last 10 signals: <40% WR â†’ DEFENSIVE MODE (raise threshold by 20 pts)",
        "Reset to normal after 20 signals OR 6 hours",
        "Add mode indicator to Telegram posts: ðŸ”¥ AGGRESSIVE or ðŸ›¡ï¸ DEFENSIVE",
        "Monitor for 48 hours",
        "Keep if: reduces losing streaks >50% (stop posting after 3 consecutive rugs)"
      ],
      "priority": 3,
      "passes": false,
      "notes": "PSYCHOLOGICAL: Don't keep posting after losses. Be aggressive when hot, defensive when cold.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-044",
      "title": "RESEARCH: Reverse-engineer graduated tokens for success patterns",
      "description": "As a researcher, I want to learn from ALL graduated tokens, not just our signals",
      "acceptanceCriteria": [
        "Scrape pump.fun API: all tokens that graduated bonding curve in last 30 days",
        "For each graduated token, fetch full history: price, volume, holder growth, KOL buys during bonding",
        "Classify WINNERS (10x+ post-grad) vs LOSERS (rug or <2x)",
        "Extract features: KOL count during bonding, holder velocity, volume pattern, liquidity additions, narrative",
        "Build success pattern database: 'Tokens with 3+ elite KOL buys during bonding + holder velocity >50/hr = 78% graduation success rate'",
        "Add graduated_token_analyzer.py that scores new signals based on grad patterns",
        "Monitor for 48 hours",
        "Keep if: win_rate improves >12% by applying grad token learnings"
      ],
      "priority": 1,
      "passes": false,
      "notes": "USER INSIGHT: Learn from the entire pump.fun ecosystem, not just our small sample size. Massive data advantage.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-045",
      "title": "RESEARCH: Analyze what DOESN'T work during bonding",
      "description": "As a risk analyst, I want to know what patterns predict failure during bonding curve",
      "acceptanceCriteria": [
        "Scrape pump.fun: all tokens that FAILED to graduate (last 30 days)",
        "Extract failure patterns: holder stagnation, volume drop-off, KOL exits, liquidity pulls",
        "Calculate failure rates per pattern: 'Tokens with volume drop >80% in first hour = 92% fail rate'",
        "Build failure fingerprint database",
        "Add bonding_failure_detector.py that blocks signals matching failure patterns >75%",
        "Monitor for 24 hours",
        "Keep if: prevents >10 rugs OR rug_rate drops >25%"
      ],
      "priority": 2,
      "passes": false,
      "notes": "USER INSIGHT: Learn what doesn't work. If pattern fails 90% of time across all pump.fun, block it immediately.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-046",
      "title": "RESEARCH: Build predictive model from graduated token dataset",
      "description": "As a data scientist, I want to train ML on ALL graduated tokens for maximum accuracy",
      "acceptanceCriteria": [
        "Build dataset: 1000+ graduated tokens with full bonding curve history",
        "Features: KOL count, holder velocity, volume curve, narrative, timing, liquidity pattern",
        "Labels: SUCCESS (graduated + 5x post-grad) vs FAILURE (didn't graduate or rugged)",
        "Train XGBoost/RandomForest on this massive dataset",
        "Model predicts: P(graduation) and P(10x post-grad)",
        "Apply to new signals: +30 conviction if P(graduation) > 80% AND P(10x) > 60%",
        "Block signals if P(graduation) < 40%",
        "Monitor for 7 days",
        "Keep if: win_rate improves >18% AND model accuracy >72%"
      ],
      "priority": 2,
      "passes": false,
      "notes": "USER INSIGHT: Train on 1000+ graduated tokens = massive dataset advantage. Learn from entire ecosystem.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-047",
      "title": "RESEARCH: Track top 100 graduated tokens for post-grad behavior",
      "description": "As a strategist, I want to understand what happens AFTER graduation",
      "acceptanceCriteria": [
        "Identify top 100 graduated tokens by volume (last 90 days)",
        "Track post-graduation behavior: price action, holder retention, KOL holds vs exits",
        "Identify SUSTAINABLE patterns: 'Tokens where 70%+ KOLs held post-grad = 3.2x avg vs 1.4x when KOLs exited'",
        "Add post_grad_predictor.py that estimates sustainability",
        "Bonus points for signals likely to have sustainable post-grad run",
        "Monitor for 14 days",
        "Keep if: avg_ROI on graduated signals improves >25%"
      ],
      "priority": 3,
      "passes": false,
      "notes": "USER INSIGHT: Graduation isn't the end - understand what makes tokens run POST-graduation.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-048",
      "title": "RESEARCH: Auto-discover new KOLs from graduated token buyers",
      "description": "As a KOL hunter, I want to find wallets that consistently buy tokens that graduate",
      "acceptanceCriteria": [
        "For all graduated tokens (last 60 days), identify wallets that bought during bonding",
        "Calculate 'graduation rate' per wallet: % of their buys that graduated",
        "Identify HIDDEN GEMS: wallets with >65% grad rate + >20 buys + not in our list",
        "Auto-add to curated_wallets.py as 'grad_sniper' tier",
        "Weight these wallets: +25 conviction pts (proven grad ability)",
        "Monitor for 14 days",
        "Keep if: new wallets contribute >5 winning signals OR win_rate improves >8%"
      ],
      "priority": 2,
      "passes": false,
      "notes": "USER INSIGHT: Find wallets we're missing by looking at who consistently buys winners. Data-driven KOL discovery.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-049",
      "title": "RESEARCH: Bonding curve velocity as predictor",
      "description": "As an analyst, I want to know if fast bonding = sustainable vs slow bonding = rug",
      "acceptanceCriteria": [
        "Analyze 500+ graduated tokens: time to graduate (fast <30min, medium 30-120min, slow >120min)",
        "Compare post-grad performance by velocity: fast vs medium vs slow",
        "Calculate win_rate and avg_ROI per velocity bucket",
        "If pattern found (e.g., 'medium velocity 30-90min = 71% WR vs fast <30min = 48% WR')",
        "Add bonding_velocity_scorer.py that adjusts conviction based on velocity",
        "Monitor for 48 hours",
        "Keep if: win_rate improves >10% by favoring optimal velocity range"
      ],
      "priority": 3,
      "passes": false,
      "notes": "USER INSIGHT: Reverse-engineer timing - is fast bonding good or bad? Let data decide.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-050",
      "title": "CONTINUOUS: Auto-update dataset weekly with new graduated tokens",
      "description": "As a learning system, I want fresh data every week to stay current with meta",
      "acceptanceCriteria": [
        "Schedule weekly scrape: all new graduated tokens from last 7 days",
        "Update success/failure pattern databases",
        "Retrain ML models on updated dataset",
        "Track meta shifts: 'narratives that worked last month vs this month'",
        "Auto-adjust scoring weights based on recent data",
        "Log changes to progress.txt: 'Meta shift detected: AI narrative win_rate dropped from 68% â†’ 42%, reducing weight'",
        "Monitor continuously",
        "Keep permanently (continuous learning)"
      ],
      "priority": 4,
      "passes": false,
      "notes": "USER INSIGHT: Meta changes weekly. Continuous learning from graduated tokens keeps us current.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-051",
      "title": "CRITICAL: Fix silent Telegram posting failures",
      "description": "As a user, I want to know when signals fail to post to Telegram instead of silent failures",
      "acceptanceCriteria": [
        "Debug: Signal with 55 conviction passed threshold but didn't post to TG",
        "Check publishers/telegram.py for silent exception handling",
        "Add explicit error logging: 'FAILED TO POST SIGNAL: {mint} - {error}'",
        "Add retry logic: 3 attempts with 2s delay between",
        "Add fallback: if TG fails, log to database with 'posting_failed' flag",
        "Add health check: alert if 3+ consecutive posting failures",
        "Monitor for 2 hours",
        "Keep if: 0 silent failures AND all passing signals post successfully"
      ],
      "priority": 0,
      "passes": false,
      "notes": "IMPLEMENTED - Waiting for deployment and monitoring. Commit: 78d7c4e. PR: https://github.com/Sydneyanon/SENTINEL_V2/compare/main...ralph/optimize-v1",
      "baseline_metrics": {},
      "implementation_status": "deployed_pending_monitoring",
      "deployment_commit": "78d7c4e",
      "monitoring_start": null,
      "monitoring_duration_hours": 2
    }
  ]
}
