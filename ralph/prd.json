{
  "project": "Prometheus Signals Bot",
  "branchName": "ralph/optimize-v1",
  "description": "Autonomous optimization of Prometheus memecoin signals bot",
  "userStories": [
    {
      "id": "OPT-001",
      "title": "Optimize conviction score threshold",
      "description": "As a trader, I want higher quality signals by tuning the minimum conviction threshold",
      "acceptanceCriteria": [
        "Test conviction thresholds: 65, 70, 75, 80",
        "Monitor for 2 hours after deploy to Railway",
        "Measure: signal count, avg ROI, false positive rate",
        "Keep threshold that maximizes (ROI * signal_count) while keeping false positives < 30%",
        "Update config.py with optimal value",
        "Commit if metrics improve by >10%"
      ],
      "priority": 1,
      "passes": false,
      "notes": "",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-002",
      "title": "Reduce Helius API credit waste",
      "description": "As an operator, I want to minimize Helius credit usage without sacrificing signal quality",
      "acceptanceCriteria": [
        "Identify top 3 credit-consuming operations from logs",
        "Increase caching TTL for holder checks (60min → 120min)",
        "Monitor credit usage for 2 hours",
        "Ensure signal quality doesn't drop >5%",
        "Commit if credits reduced by >20%"
      ],
      "priority": 2,
      "passes": false,
      "notes": "",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-003",
      "title": "Auto-discover high-performing wallets",
      "description": "As a system, I want to automatically add wallets that consistently call winners",
      "acceptanceCriteria": [
        "Query database for wallets that appeared in 3+ successful signals (>2x ROI)",
        "Filter for win rate >70%",
        "Add top 5 to curated_wallets.py as 'discovered' tier",
        "Monitor for 4 hours",
        "Keep if new wallets generate >1 successful signal",
        "Commit wallet additions with performance data"
      ],
      "priority": 3,
      "passes": false,
      "notes": "",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-004",
      "title": "Tune bundle detection penalties",
      "description": "As a filter, I want optimal bundle penalties that catch rugs without false positives",
      "acceptanceCriteria": [
        "Test bundle penalty values: -10, -15, -20 for minor bundles",
        "Monitor rug detection accuracy for 2 hours",
        "Measure: rugs caught, false positives (organic pumps blocked)",
        "Optimize for max rugs caught with <10% false positive rate",
        "Update RUG_DETECTION config",
        "Commit if rug catch rate improves >15%"
      ],
      "priority": 4,
      "passes": false,
      "notes": "",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-005",
      "title": "Build ML prediction layer",
      "description": "As a system, I want to learn from historical signals to predict success",
      "acceptanceCriteria": [
        "Extract features from database: smart_wallet_count, narrative_match, holder_concentration, volume_ratio",
        "Train simple logistic regression on historical signals (target: 2x+ ROI)",
        "Add ML score as bonus points (0-10) in conviction_engine.py",
        "Monitor for 4 hours",
        "Keep if signal success rate improves >10%",
        "Commit model weights and integration code"
      ],
      "priority": 5,
      "passes": false,
      "notes": "",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-006",
      "title": "Build on-chain data pipeline for proprietary signals",
      "description": "As a system, I want real-time Solana on-chain data to detect smart money moves before public APIs",
      "acceptanceCriteria": [
        "Implement Helius Geyser websocket streamer for new token launches (Pump.fun)",
        "Store 10k+ transactions to database (wallet, token, amount, timestamp)",
        "Add data processing: cluster wallets by behavior (early buyers, quick flippers)",
        "Create new signal source: 'on_chain_clusters' in conviction engine",
        "Monitor for 4 hours",
        "Keep if discovers 1+ high-performing wallet not in curated list",
        "Commit streaming code + database schema"
      ],
      "priority": 6,
      "passes": false,
      "notes": "Grok idea: Build data moat with on-chain streaming",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-007",
      "title": "Add RSS narrative detection with BERTopic ML",
      "description": "As a narrative detector, I want to discover emerging narratives from crypto news without X API costs",
      "acceptanceCriteria": [
        "Implement RSS feed ingestion: CoinDesk, CoinTelegraph, Decrypt (free)",
        "Add BERTopic clustering: hourly analysis for topic trends",
        "Detect 'emerging' narratives: new clusters with >5 mentions in 24h",
        "Add Reddit PRAW scraper for r/Solana hot posts (optional)",
        "Auto-update HOT_NARRATIVES in config.py with discovered topics",
        "Monitor for 24 hours",
        "Keep if discovers 1+ narrative that generates successful signal",
        "Commit RSS pipeline + BERTopic model"
      ],
      "priority": 7,
      "passes": false,
      "notes": "Grok idea: Free narrative data via RSS + ML clustering",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-008",
      "title": "ML-powered smart wallet discovery from on-chain data",
      "description": "As a wallet curator, I want ML to automatically find high-alpha wallets from transaction patterns",
      "acceptanceCriteria": [
        "Query Dune/Helius for Solana traders: 100+ trades, meme focus",
        "Extract features: win_rate, avg_roi, hold_time, entry_timing, trade_frequency",
        "Train scikit-learn classifier: 'smart' = win_rate>70% AND avg_roi>50%",
        "Apply to unlabeled wallets: predict top 20 candidates",
        "Add top 5 to curated_wallets.py as 'ml_discovered' tier",
        "Monitor for 7 days",
        "Keep if ML wallets generate 2+ successful signals",
        "Commit ML model + discovered wallets"
      ],
      "priority": 8,
      "passes": false,
      "notes": "Grok idea: Proprietary smart wallet list via ML clustering",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-009",
      "title": "Build backtesting framework for strategy validation",
      "description": "As a system, I want to validate optimizations on historical data before deploying",
      "acceptanceCriteria": [
        "Create backtesting engine: replay historical signals from database",
        "Simulate conviction scoring with different thresholds/weights",
        "Measure historical ROI, win_rate, drawdowns for each config",
        "Add test suite: backtest OPT-001 through OPT-005 on last 30 days",
        "Generate report: best config = highest Sharpe ratio",
        "Integrate into ralph/collect_metrics.py for future optimizations",
        "Commit backtesting framework"
      ],
      "priority": 9,
      "passes": false,
      "notes": "Grok idea: Backtest optimizations before live deploy",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-010",
      "title": "Add dynamic risk management with ML exit strategies",
      "description": "As a trader, I want automated TP/SL that adapts to volatility and market conditions",
      "acceptanceCriteria": [
        "Implement volatility calculation: rolling std dev from price data",
        "Train ML model for dynamic exits: features (volatility, volume_spike, holder_growth)",
        "Add adaptive trailing stop: tighter in high volatility, wider in calm markets",
        "Create exit signal generator in conviction_engine.py",
        "Backtest on 100 historical signals: measure improvement in avg_roi",
        "Monitor live for 7 days",
        "Keep if avg_roi improves >15% or max drawdown reduces >20%",
        "Commit ML exit model + integration"
      ],
      "priority": 10,
      "passes": false,
      "notes": "Grok idea: ML-powered dynamic TP/SL for better exits",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-011",
      "title": "Track Telegram call group accuracy (like outlight.fun)",
      "description": "As a signal source, I want to monitor Telegram groups and score them by accuracy",
      "acceptanceCriteria": [
        "Use Telethon/Pyrogram to monitor 10-20 popular Solana call groups",
        "Parse call messages: extract token CA, timestamp, entry price",
        "Track outcomes: did it pump? max ROI reached?",
        "Store to database: group_id, token, call_time, outcome, ROI",
        "Calculate caller accuracy: % of calls that 2x'd, avg ROI",
        "Add top 3 accurate groups as signal source in conviction engine (+5-15 pts)",
        "Monitor for 7 days",
        "Keep if group calls generate 2+ successful signals",
        "Commit Telegram monitoring code + caller scoring"
      ],
      "priority": 11,
      "passes": false,
      "notes": "Replicate outlight.fun caller tracking - monitor TG groups, score by accuracy",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-012",
      "title": "Build ML learning engine to identify high market cap conditions",
      "description": "As a learning system, I want to understand what conditions lead to tokens hitting high market caps (100k+, 1M+, 10M+)",
      "acceptanceCriteria": [
        "Extract features from all tracked tokens: holder_concentration_top10, holder_concentration_top3, volume_to_mcap_ratio, kol_count, kol_tier_distribution, unique_buyers_first_hour, price_momentum_5m, price_momentum_1h, liquidity_depth, narrative_match_count, bundle_detected, early_buyer_count, avg_entry_time, holder_growth_rate",
        "Label outcomes: 0=rug, 1=2x, 2=10x, 3=50x, 4=100x+",
        "Train gradient boosting model (XGBoost) to predict outcome class",
        "Feature importance analysis: which signals matter most for 100x?",
        "Generate insights: 'Tokens with 3+ elite KOLs + <30% top-10 holders + AI narrative = 65% chance of 10x+'",
        "Add ML predictions to conviction engine: +0-20 points based on predicted outcome",
        "Retrain weekly on new data (continuous learning)",
        "Monitor for 14 days",
        "Keep if prediction accuracy >70% and signals improve >15%",
        "Commit ML model, feature extractor, training pipeline, insights dashboard"
      ],
      "priority": 12,
      "passes": false,
      "notes": "Meta-learning: Learn what conditions = high mcap. Features: holders%, volume, KOLs, narratives, momentum, etc.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-013",
      "title": "Auto-detect and fix runtime errors from Railway logs",
      "description": "As a self-healing system, I want to detect errors in Railway logs and automatically fix them",
      "acceptanceCriteria": [
        "Implement Railway API log fetcher: pull last 1000 lines of logs",
        "Parse for error patterns: 'ERROR', 'Exception', 'Failed', 'Traceback'",
        "Classify errors: API failures, data parsing, missing data, timeouts",
        "For each error type, implement fix: add try/catch, add validation, add fallbacks",
        "Test fixes locally, then deploy to Railway",
        "Monitor for 2 hours to ensure error rate drops",
        "Keep if error count decreases >50%",
        "Commit fixes with error analysis in commit message"
      ],
      "priority": 13,
      "passes": false,
      "notes": "User request: Ralph should fix errors automatically. High priority!",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-014",
      "title": "Optimize metadata collection (name, symbol, description)",
      "description": "As a data pipeline, I want faster and more reliable token metadata without wasting credits",
      "acceptanceCriteria": [
        "Audit current metadata sources: Helius DAS API, bonding curve, DexScreener",
        "Measure success rate and latency for each source",
        "Add fallback chain: Helius → DexScreener → Jupiter API → Solscan",
        "Implement 24h cache for token metadata (Redis or in-memory)",
        "Add parallel fetching: fetch metadata + price + holders simultaneously",
        "Monitor for 2 hours",
        "Keep if metadata fetch success rate >95% and latency <500ms",
        "Commit optimized metadata pipeline"
      ],
      "priority": 14,
      "passes": false,
      "notes": "User request: Optimize metadata collection. Name/symbol often missing.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-015",
      "title": "Optimize price data fetching (reduce 'No price data' errors)",
      "description": "As a pricing system, I want accurate prices from multiple sources with smart fallbacks",
      "acceptanceCriteria": [
        "Audit price sources: bonding curve decoder, DexScreener, Jupiter, Raydium",
        "Measure success rate per source and avg price accuracy",
        "Implement weighted price aggregation: average prices from multiple sources",
        "Add staleness detection: reject prices >5 minutes old",
        "Implement price cache: 30s TTL to reduce API calls",
        "Add sanity checks: reject outliers (price changes >500% in 1 min)",
        "Monitor for 2 hours",
        "Keep if 'No price data' errors drop >80%",
        "Commit improved price pipeline"
      ],
      "priority": 15,
      "passes": false,
      "notes": "User request: Fix price fetching. Often shows 'No price data available'.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-016",
      "title": "Track and optimize KOL performance (win rate, ROI, accuracy)",
      "description": "As a wallet curator, I want to track which KOLs are actually making money and adjust scoring",
      "acceptanceCriteria": [
        "For each KOL wallet, track: tokens bought, outcomes (rug/2x/10x/50x+), win_rate, avg_roi",
        "Store to database: kol_performance table with weekly aggregates",
        "Auto-demote KOLs with <50% win rate after 20+ trades",
        "Auto-promote wallets with >75% win rate + >3x avg ROI",
        "Adjust scoring weights: high-performing KOLs get +15 pts, low-performers get +5 pts",
        "Create /kol-leaderboard command: show top 10 KOLs by performance",
        "Monitor for 7 days",
        "Keep if signal quality improves >10%",
        "Commit KOL performance tracking system"
      ],
      "priority": 16,
      "passes": false,
      "notes": "User request: Track KOL win rates and optimize scoring based on actual performance.",
      "baseline_metrics": {}
    },
    {
      "id": "OPT-017",
      "title": "Auto-tune scoring weights with ML gradient descent",
      "description": "As a self-optimizing system, I want to learn optimal scoring weights from historical data",
      "acceptanceCriteria": [
        "Extract last 100 signals from database with outcomes (rug/2x/10x/etc)",
        "Current weights: smart_wallet=40, narrative=25, holders=15, volume=10, momentum=10",
        "Train ML model (gradient descent) to predict outcome from feature scores",
        "Find optimal weights that maximize 10x+ predictions while minimizing rugs",
        "Test on validation set: compare old weights vs new weights",
        "Apply new weights to config.py",
        "Monitor for 3 days",
        "Keep if 10x+ rate improves >20% OR rug rate drops >30%",
        "Commit optimized weights with ML analysis report"
      ],
      "priority": 17,
      "passes": false,
      "notes": "User request: Optimize scoring algorithm. Let ML find best weights.",
      "baseline_metrics": {}
    }
  ]
}
